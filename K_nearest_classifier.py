# use warnings to avoid using a lower K value than we have groups
# use math for the square root functionality
# counter from collections to get the most popular votes
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
from matplotlib import style
import warnings
from math import sqrt
from collections import Counter
style.use('fivethirtyeight')


# create a k nearest neighbors classifiers
def k_nearest_neightbors(data, predict,k=3):
    if len(data)>=k:
        warnings.warn('K is set to a value less than total groups!')
    distances = []
    for group in data:
        for features in data[group]:
            # euclidean_distance = sqrt((features[0]-predict[0])**2 +
            #                           (features[1]-predict[1])**2 )
            euclidean_distance = np.sqrt((np.sum((np.array(features)-np.array(predict))**2)))
            distances.append([euclidean_distance,group])

    votes = [i[1] for i in sorted(distances)[:k]]
    vote_result = Counter(votes).most_common(1)[0][0]
    return vote_result

# for group in dataset:
#     for features in dataset[group]:
#         print("group:",group,"Features:",features)

# Output is :
# group: r Features: [6, 5]
# group: r Features: [7, 7]
# group: r Features: [8, 6]
# group: k Features: [1, 2]
# group: k Features: [2, 3]
# group: k Features: [3, 2]

################################
# Data set generated by ourself#
################################
# # the keys are the color of the points
# dataset = {'k':[[1,2],[2,3],[3,2]], 'r':[[6,5],[7,7],[8,6]]}
# new_features = [5,7]
#
# # create a graph
# [[plt.scatter(ii[0],ii[1],s=50,color=i) for ii in dataset[i]] for i in dataset]
# plt.scatter(new_features[0], new_features[1],s=50)
# # plt.show()
#
# # for i in dataset:
# #     for ii in dataset[i]:
# #         plt.scatter(ii[0],ii[i],s=100,color=i)
# result = k_nearest_neightbors(dataset,new_features)
# print("results: ", result)
#
# plt.scatter(new_features[0],new_features[1],s=50,color=result)
# plt.show()

#############################
#Import data set form cancer#
#############################
df = pd.read_csv('breast-cancer-wisconsin.data')
df.replace('?',-99999,inplace=True)
df.drop(['id'],1,inplace=True)
full_data = df.astype(float).values.tolist() # convert to list of lists as float

# now shuffle the data
random.shuffle(full_data)
test_size = 0.2

# create dictionary for data set
# it has two keys: 2 and 4. the 2 is for the benign tumors
# the4 is for malignant tumors.
train_set = {2:[], 4:[]}
test_set = {2:[], 4:[]}
length_data= len(full_data)
# select the first 80% as train_data, the final 20% of test_data
train_data = full_data[:-int(test_size*length_data)]
test_data = full_data[-int(test_size*length_data):]

# append the features into train set
for i in train_data:
    train_set[i[-1]].append(i[:-1])
for i in test_data:
    test_set[i[-1]].append(i[:-1])

correct = 0
total = 0

# this is the way for loop of dictionary
for group in test_set:
    for data in test_set[group]:
        vote = k_nearest_neightbors(train_set, data, k=5)
        if group == vote:
            correct += 1
        total +=1

print("Accuracy:{0:.1%}".format(correct/total))

# The downfall is in scale, with outliers, and with any bad data